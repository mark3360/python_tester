{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54761916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade jupyter_client testbook pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c7beb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from testbook import testbook\n",
    "import time\n",
    "from timeout_decorator import timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e4e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up things need for every test\n",
    "\n",
    "\n",
    "#os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "\n",
    "class Test:\n",
    "    def __init__(self, name, time, expected, close, full_marks, part_marks, test):\n",
    "        self.name = name\n",
    "        self.time = time\n",
    "        self.expected = expected\n",
    "        self.close = close\n",
    "        self.full_marks = full_marks\n",
    "        self.part_marks = part_marks\n",
    "        self.the_test = test #Lambda expression that takes in a function and tests that function\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Tester:\n",
    "    feedback = \"\"\n",
    "    marks_earned = 0\n",
    "    total_marks = 0\n",
    "    \n",
    "    #expected is \"correct\" result, close is a list for awarding part marks.\n",
    "    def run_test(self, student_function, test):\n",
    "        self.total_marks += test.full_marks\n",
    "        marks_awarded = 0\n",
    "        self.feedback += f\"{test.name}:\" \n",
    "        \n",
    "        if student_function == None:\n",
    "            self.feedback += \" Function Undefined!\"\n",
    "        else:\n",
    "            try:\n",
    "                @timeout(test.time)\n",
    "                def f():\n",
    "                    return test.the_test(student_function)\n",
    "                result = f()\n",
    "                \n",
    "            except TimeoutError as e:\n",
    "                self.feedback +=  f\" Function {student_function.__name__} timed out with time limit {test.time}\"\n",
    "            except:\n",
    "                self.feedback += f\" Function {student_function.__name__} encountered error while running\"\n",
    "            \n",
    "            if result == test.expected:\n",
    "                self.feedback += \" Test Passed!\"\n",
    "                marks_awarded = test.full_marks\n",
    "            elif result in test.close:\n",
    "                self.feedback += \" Function contains a slight error\"\n",
    "                marks_awarded = test.part_marks\n",
    "            else:\n",
    "                self.feedback += f\" Test Failed, expected {test.expected}, but saw {result}\"\n",
    "            \n",
    "            self.marks_earned += marks_awarded\n",
    "            self.feedback += \"\\n\"\n",
    "            self.feedback += f\"{marks_awarded} / {test.full_marks}\"\n",
    "            self.feedback += \"\\n\"\n",
    "            self.feedback += \"\\n\"\n",
    "            \n",
    "\n",
    "def sanitize(file):\n",
    "    file = open(file, \"r\")\n",
    "    text = file.read()\n",
    "    res = re.findall('\\sos[\\s|\"]', text)\n",
    "    res += re.findall('\\ssys[\\s|\"]', text)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa639d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tests here.\n",
    "# Order is name, time, expected, close, full_marks, part_marks, test\n",
    "\n",
    "data1 = {'Bond Return' : 1000,\n",
    "     'Bond Price' : 960,\n",
    "     'Chance of Poor Economy' : 0.4,\n",
    "     'A in Poor Economy' : 700,\n",
    "     'A in Good Economy' : 1200,\n",
    "     'B in Good Economy' : 200,\n",
    "     'C in Poor Economy' : 300,\n",
    "     'A Risk Premium' : 0.1}\n",
    "\n",
    "direct_test = lambda f : f(data1) #Used for directly checking output\n",
    "\n",
    "round_test = lambda f : round(f(data1), 4)\n",
    "\n",
    "test1 = Test(\"risk_free_rate1\", 10, 0.0417, [], 1, 0, round_test)\n",
    "test2 = Test(\"Test2\", 10, 1000, [], 1, 0, round_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48542f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing i8zhao\n",
      "Feedback for i8zhao:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = \"A1/\"\n",
    "\n",
    "#grades = pd.read_csv(\"grades.csv\")\n",
    "#breakdown = pd.DataFrame(data = {\"watiam\" : [],\n",
    "#                                   \"score\" : [],\n",
    "#                                   \"total\" : [],})\n",
    "\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    lst = filename.split(\" - \")\n",
    "    (file_id, Name, watiam, file) = lst\n",
    "    path = directory + filename\n",
    "    illegal_phrases = sanitize(path)\n",
    "    \n",
    "    print(f\"Testing {watiam}\")\n",
    "    if illegal_phrases != []:\n",
    "        print(f\"Testing of file {filename} aborted due to illegal phrase(s)\")\n",
    "        print(illegal_phrases)\n",
    "    \n",
    "    \n",
    "    feedback = f\"Feedback for {watiam}:\\n\\n\"\n",
    "\n",
    "    t = Tester()\n",
    "    \n",
    "    \n",
    "        \n",
    "    #Attempt to load all student functions\n",
    "    try:\n",
    "        s_risk_free_rate = risk_free_rate\n",
    "    except:\n",
    "        s_risk_free_rate = None\n",
    "    try:\n",
    "        s_payoff_A = payoff_A\n",
    "    except:\n",
    "        s_payoff_A = None\n",
    "        \n",
    "        the_tester.run_test(s_risk_free_rate, test1)\n",
    "        the_tester.run_test(s_payoff_A, test2)\n",
    "\n",
    "    theTest(t)\n",
    "    feedback = t.feedback\n",
    "    total = t.total_marks\n",
    "    score = t.marks_earned\n",
    "    new_row = {'watiam': watiam, 'score': score, 'total': total}\n",
    "    breakdown.loc[len(breakdown)] = new_row\n",
    "    print(feedback)\n",
    "    break\n",
    "  \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8ac6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
